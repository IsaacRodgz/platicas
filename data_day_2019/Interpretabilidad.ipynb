{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/DataDay-logo.png)\n",
    "<br>\n",
    "\n",
    "## Interpretabilidad en modelos de aprendizaje de máquina\n",
    "\n",
    "+ M.Sc. Liliana Millán, liliana@datank.ai\n",
    "+ Twitter: @silil3\n",
    "\n",
    "\n",
    "[https://sg.com.mx/dataday](https://sg.com.mx/dataday) \\#DataDayMx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "+ Interpretabilidad\n",
    "+ Interpretabilidad en ML\n",
    "+ LIME\n",
    "+ Casos de uso\n",
    "+ Ejemplo\n",
    "+ Otros métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretabilidad\n",
    "\n",
    "+ Interpretabilidad: El grado en el que un **humano** puede entender la causa de una decisión.\n",
    "+ Nos permite identificar y evitar tener sesgo, injusticia, inequidad en los modelos que generamos.\n",
    "\n",
    "**Ejemplos:** \n",
    "\n",
    "+ COMPAS (US): Predecir si alguien reincidirá en cometer un delito.\n",
    "+ PredPol (US): Predecir cuándo y en dónde habrá un crimen. El algoritmo aprende de reportes de policias, no necesariamente crímenes cometidos.\n",
    "+ *Facial Recognition* (IBM, Microsoft, Megvii): Reconocimiento para hombres blancos 99%, mujeres afroamericanas 35% ...\n",
    "+ Búsqueda de imágenes en Google para CEO: Solo muestra a mujeres 11%.\n",
    "+ *Facebook automatic translation:* Arresto de un palestino por traducción incorrecta de \"buenos días\" en hebreo a \"atácalos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algo más personal\n",
    "\n",
    "... 19S\n",
    "\n",
    "Tener interpretabilidad en el modelo nos brinda más confianza en que lo estamos haciendo correctamente -además de las métricas de desempeño-.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretabilidad en ML\n",
    "\n",
    "+ GDPR: *\"Right to explanation\"* (Mayo 2018).\n",
    "+ Aplicable a modelos de aprendizaje supervisado.\n",
    "+ El chisme: es ilegal ocupar *Deep Learning* en Europa.\n",
    "+ Preferimos ocupar Regresión lineal, logística, árboles porque nos permiten entender por qué se están tomando esas decisiones.\n",
    "+ *Why should I trust you?* (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LIME\n",
    "\n",
    "*Local Interpertable Model-Agnostic Explanation*\n",
    "\n",
    "**Objetivo:** Tener explicaciones que un humano pueda comprender de cualquier modelo (supervisado) a través de un modelo local más simple.\n",
    "\n",
    "**Suposición:** Un modelo complejo es lineal en una escala local.\n",
    "\n",
    "+ *Model Agnostic:* Tratamos al modelo \"complejo\" utilizado como una caja negra a la que metemos observaciones y nos entrega predicciones.\n",
    "+ *Interpretable:* Que la explicación pueda ser entendida por un humano.\n",
    "+ *Local:* Un modelo simple es lo suficientemente bueno/igual de bueno localmente que uno complejo globalmente ([Why should I trust you?](https://arxiv.org/pdf/1602.04938.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LIME \n",
    "\n",
    "![](images/lime.png)\n",
    "<br>\n",
    "Fuente: [Why should I trust you?](https://arxiv.org/pdf/1602.04938.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo funciona?\n",
    "\n",
    "    \n",
    "Ingeniería inversa: Modificamos las entradas, las pasamos por la \"caja negra\", observamos los resultados.\n",
    "\n",
    "![](images/frog_lime.png)\n",
    "<br>\n",
    "Fuente: [Local Interpretable Model-Agnostic Explanations (LIME): An Introduction](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo funciona?\n",
    "\n",
    "![](images/huskie_lime.png)\n",
    "<br>\n",
    "[Why should I trust you?](https://arxiv.org/pdf/1602.04938.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo funciona?\n",
    "\n",
    "![](images/lime_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Cómo funciona?\n",
    "\n",
    "+ Vecindad\n",
    "+ Similitud\n",
    "+ Selección de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beneficios\n",
    "\n",
    "+ Identificar si podemos confiar en las predicciones de un modelo como científico de datos.\n",
    "+ Generar confianza al usuario sobre las predicciones que genera un modelo.\n",
    "+ Brindar información para identificar si hay sesgo/inequidad/injusticia en nuestro modelo.\n",
    "+ LIME se puede ocupar en R y Python :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Desventajas\n",
    "\n",
    "+ Definir la vecindad adecuada para generar el modelo lineal.\n",
    "+ Para junio 2018 solo era posible ocupar regresión lineal (ridge) como modelo simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Casos de Uso\n",
    "\n",
    "+ Validación de desempeño entre diferentes modelos.\n",
    "+ Interpretabilidad en modelos complejos: SVM, RF, Redes Neuronales: Clásicas, Profundas, Convolucionales, Recurrentes, etc.\n",
    "+ Brindarle una explicación al usuario/afectado final.\n",
    "+ El algoritmo de aprendizaje tiene que ser supervisado.\n",
    "+ Los datos pueden ser tabulares, imágenes o texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo \n",
    "\n",
    "Una microfinanciera necesita un modelo de machine learning que le permita establecer el nivel óptimo de crédito que cada uno de sus clientes debe tener:\n",
    "\n",
    "+ Minimizando el riesgo de caer en mora.\n",
    "+ Maximizando la oportunidad de ocupar la mayoría de su crédito.\n",
    "\n",
    "El modelo puede emitir 1 de 3 posibles recomendaciones: **aumentar**, **disminuir** o dejar su nivel de crédito **igual**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo \n",
    "\n",
    "+ Opción aumentar: sin problema para el cliente.\n",
    "+ Opción dejar igual: sin problema para el cliente.\n",
    "+ Opción disminuir: altamente probable que el cliente **exiga** una explicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "explainer_rf = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=train_data_selected.values, #el set de entrenamiento con el que se entreno el rf\n",
    "    mode=\"classification\",\n",
    "    feature_names=feature_names, #nombre de las variables, en el orden que se envian\n",
    "    class_names=class_names,#el nombre de las etiquetas de clasificación\n",
    "    kernel_width=0.75) #el tamaño de la ventana para generar datos (mientras más pequeño más cercano a los valores reales)\n",
    "                                                \n",
    "#single prediction\n",
    "sp = explainer_rf.explain_instance(test_obs_1, modelo_lco.predict_proba, num_features=10)\n",
    "#sp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/explanation_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sp_2 = explainer_rf.explain_instance(test_obs_2, modelo_lco.predict_proba, num_features=10)\n",
    "#sp_2.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/explanation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo \n",
    "\n",
    "+ LIME nos permite tener interpretabilidad por observación\n",
    "+ Tenemos alrededor de 5k observaciones a aplicar un modelo local\n",
    "+ Se necesita hacer un *pipeline* para correr esos modelos en paralelo\n",
    "\n",
    "![](images/user_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Otros métodos para interpretar\n",
    "\n",
    "+ [*Partial Dependence Plot* (PDP)](https://christophm.github.io/interpretable-ml-book/agnostic.html). Efecto marginal de una o dos variables en la predicción.\n",
    "+ [*Shapley Values*](https://christophm.github.io/interpretable-ml-book/agnostic.html). *Cooperative Game Theory*. Asignar una recompensa a un jugador dependiendo de su contribución.\n",
    "+ [*Accumulated Local Effects* (ALE)](https://christophm.github.io/interpretable-ml-book/agnostic.html). Cómo las variables influyen la predicción en promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algunos consejos \n",
    "\n",
    "+ Trata cada modelo que desarrollas como una afectación de vida o muerte a un humano directamente.\n",
    "+ Identifica si estas agregando sesgo, inequidad, injusticia con interpretabilidad.\n",
    "+ Siempre que tengas un modelo de aprendizaje supervisado ocupa interpretabilidad en tu proceso de desarrollo.\n",
    "+ Siempre que tengas un modelo de aprendizaje supervisado genera interpretabilidad para el usuario final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ¿Preguntas?\n",
    "\n",
    "+ M.Sc. Liliana Millán, liliana@datank.ai\n",
    "+ Twitter: @silil3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Referencias\n",
    "\n",
    "+ [LIME](https://github.com/marcotcr/lime)\n",
    "+ [Why should I trust you?](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "+ [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
